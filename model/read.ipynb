{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata, metadata_io,dataset_schema \n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/Niklas/Documents/tf_ner/models'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HAVE TO RE RUN ALL! AND CLEAR FOLDERS\n",
    "\n",
    "https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(\n",
    "    os.path.join(\n",
    "      \"gs://named_entity_recognition/beam/\", transform_fn_io.TRANSFORMED_METADATA_DIR\n",
    "      )\n",
    "    )\n",
    "transformed_feature_spec = transformed_metadata.schema.as_feature_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'chars': FixedLenFeature(shape=[30, 10], dtype=tf.int64, default_value=None),\n",
       " u'chars_in_word': FixedLenFeature(shape=[30], dtype=tf.int64, default_value=None),\n",
       " u'id': FixedLenFeature(shape=[], dtype=tf.float32, default_value=None),\n",
       " u'label': FixedLenFeature(shape=[], dtype=tf.string, default_value=None),\n",
       " u'label_length': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " u'labels': FixedLenFeature(shape=[30], dtype=tf.string, default_value=None),\n",
       " u'sentence_length': FixedLenFeature(shape=[], dtype=tf.int64, default_value=None),\n",
       " u'text': FixedLenFeature(shape=[], dtype=tf.string, default_value=None),\n",
       " u'word_representation': VarLenFeature(dtype=tf.int64)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_feature_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _read_from_tfrecord(example_proto):\n",
    "    features = tf.parse_example([example_proto], features=transformed_feature_spec)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.data.TFRecordDataset(\"gs://named_entity_recognition/beam/TEST-00000-of-00002\").map(_read_from_tfrecord).batch(2)\n",
    "for x in ds:\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import ascii_lowercase\n",
    "MAPPING = {a:index for index,a in enumerate(ascii_lowercase + ascii_lowercase.upper())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=84, shape=(2, 1), dtype=string, numpy=\n",
       "array([['The London march came ahead of anti-war protests today in other cities , including Rome , Paris , and Madrid .'],\n",
       "       ['Thousands of demonstrators have marched through London to protest the war in Iraq and demand the withdrawal of British troops from that country .']],\n",
       "      dtype=object)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LEN=30\n",
    "WORD_LEN=10\n",
    "BATCH_SIZE=2\n",
    "CHAR_SIZE=54\n",
    "EMBED_DIM_CHAR=int(CHAR_SIZE**(1/4))*8\n",
    "DEFAULT_WORD_VALUE =0\n",
    "VOCAB_SIZE=1e4\n",
    "EMBED_DIM_WORD=int(VOCAB_SIZE**(1/4))*8\n",
    "input_char = tf.reshape(x[\"chars\"],[BATCH_SIZE,SENTENCE_LEN,WORD_LEN])\n",
    "char_embedding = tf.contrib.layers.embed_sequence(\n",
    "    input_char, vocab_size=CHAR_SIZE, embed_dim=EMBED_DIM_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No padding needed now :) \n",
    "input_word = tf.sparse_to_dense(x[\"word_representation\"].indices,[BATCH_SIZE,1,SENTENCE_LEN],\n",
    "                           x[\"word_representation\"].values,default_value=DEFAULT_WORD_VALUE\n",
    "                          )\n",
    "input_word = tf.reshape(input_word,[BATCH_SIZE,SENTENCE_LEN])\n",
    "word_embedding = tf.contrib.layers.embed_sequence(\n",
    "    input_word, vocab_size=VOCAB_SIZE, embed_dim=EMBED_DIM_WORD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([ 2 30], shape=(2,), dtype=int32)\n",
      "tf.Tensor([ 2 30 80], shape=(3,), dtype=int32)\n",
      "tf.Tensor([ 2 30 10 16], shape=(4,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Print the shapes \n",
    "print(tf.shape(input_word))\n",
    "print(tf.shape(word_embedding))\n",
    "print(tf.shape(char_embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.shape(char_embedding)\n",
    "char_embeddings = tf.reshape(char_embedding, shape=[-1, s[-2], s[-1]])\n",
    "word_lengths = tf.reshape(x['chars_in_word'],[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAR_HIDDEN_SIZE=10\n",
    "# 3. bi lstm on chars\n",
    "cell_fw = tf.contrib.rnn.LSTMCell(CHAR_HIDDEN_SIZE, state_is_tuple=True)\n",
    "cell_bw = tf.contrib.rnn.LSTMCell(CHAR_HIDDEN_SIZE, state_is_tuple=True)\n",
    "\n",
    "_, ((_, output_fw), (_, output_bw)) = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "    cell_bw, char_embeddings, sequence_length=word_lengths,\n",
    "    dtype=tf.float32)\n",
    "#shape = (batch x sentence, 2 x char_hidden_size)\n",
    "output = tf.concat([output_fw, output_bw], axis=-1)\n",
    "\n",
    "# shape = (batch, sentence, 2 x char_hidden_size)\n",
    "char_rep = tf.reshape(output, shape=[-1, s[1], 2*CHAR_HIDDEN_SIZE])\n",
    "\n",
    "# shape = (batch, sentence, 2 x char_hidden_size + word_vector_size)\n",
    "word_embeddings = tf.concat([word_embedding, char_rep], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD LSTM:S\n",
    "\n",
    "HIDDEN_SIZE=20\n",
    "cell_fw = tf.contrib.rnn.LSTMCell(HIDDEN_SIZE)\n",
    "cell_bw = tf.contrib.rnn.LSTMCell(HIDDEN_SIZE)\n",
    "sequence_length= tf.reshape(x['sentence_length'],[BATCH_SIZE])\n",
    "(output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn(cell_fw,\n",
    "    cell_bw, word_embeddings, sequence_length=sequence_length,\n",
    "    dtype=tf.float32)\n",
    "context_rep = tf.concat([output_fw, output_bw], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1053, shape=(2,), dtype=int64, numpy=array([21, 24])>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3463, shape=(3,), dtype=int32, numpy=array([ 2, 30, 40], dtype=int32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(context_rep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASSES_OUTPUT=3\n",
    "\n",
    "ntime_steps = tf.shape(context_rep)[1]\n",
    "context_rep_flat = tf.reshape(context_rep, [-1, 2*HIDDEN_SIZE])\n",
    "preds = tf.layers.dense(inputs = context_rep_flat,units=CLASSES_OUTPUT)\n",
    "logits = tf.reshape(preds, [-1, ntime_steps, CLASSES_OUTPUT])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_params = tf.get_variable(\"crf\", [CLASSES_OUTPUT, CLASSES_OUTPUT], dtype=tf.float32)\n",
    "pred_ids, _ = tf.contrib.crf.crf_decode(logits, crf_params, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-18-825a1a96e791>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-825a1a96e791>\"\u001b[0;36m, line \u001b[0;32m13\u001b[0m\n\u001b[0;31m    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "LOOK_UP_TABLE=\"FILE_NAME_TO_LOOP_UP_VOCAB\"\n",
    "\n",
    "if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "    # Predictions\n",
    "    # GET THE WORD FORM THE ID, LINK THIS WITH THE BEAM TRANSFORM VOCAB\n",
    "    reverse_vocab_tags = tf.contrib.lookup.index_to_string_table_from_file(\n",
    "        LOOK_UP_TABLE)\n",
    "    pred_strings = reverse_vocab_tags.lookup(tf.to_int64(pred_ids))\n",
    "    predictions = {\n",
    "        'pred_ids': pred_ids,\n",
    "        'tags': pred_strings\n",
    "    }\n",
    "    return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n",
    "else:\n",
    "    # Loss\n",
    "    tags = labes\n",
    "    log_likelihood, _ = tf.contrib.crf.crf_log_likelihood(\n",
    "        logits, tags, nwords, crf_params)\n",
    "    loss = tf.reduce_mean(-log_likelihood)\n",
    "\n",
    "    # Metrics\n",
    "    # AWESOME THIS IS GREAT TO USE THIS MASK HERE\n",
    "    # WILL SOLVE A LOT OF PROBLEMS, OTHERWISE\n",
    "    # GOOD LEARNING AS WELL\n",
    "    weights = tf.sequence_mask(nwords)\n",
    "    metrics = {\n",
    "        'acc': tf.metrics.accuracy(tags, pred_ids, weights),\n",
    "        'precision': precision(tags, pred_ids, num_tags, indices, weights),\n",
    "        'recall': recall(tags, pred_ids, num_tags, indices, weights),\n",
    "        'f1': f1(tags, pred_ids, num_tags, indices, weights),\n",
    "    }\n",
    "    # ADD THEM ALL TO THE GRAPH\n",
    "    for metric_name, op in metrics.items():\n",
    "        tf.summary.scalar(metric_name, op[1])\n",
    "    \n",
    "    # WHAT WILL HAPPEN IF WE HAVE EVAL MODE\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "    # WHAT WILL HAPPEN IF WE HAVE TRAIN? \n",
    "    elif mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        train_op = tf.train.AdamOptimizer().minimize(\n",
    "            loss, global_step=tf.train.get_or_create_global_step())\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, train_op=train_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
